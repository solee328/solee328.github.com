---
layout: post
title: 처음부터 시작하는 GAN(Generative Adversarial Networks) 논문 톺아보기
# subtitle:
categories: gan
tags: [gan, cnn]
# sidebar: []
use_math: true
---

## 0. GAN?
아마 이 글을 보시는 대부분의 분들은 GAN에 대해서 이미 지식이 있으실 것 같다는 생각이 듭니다. 저 또한 다른 분의 블로그를 통해서 GAN 논문을 살펴보았던 기억이 있습니다ㅎㅎ. 너무나도 유명한 모델이고 빠르게 발전하고 있는 GAN 모델들의 초석이 된 논문인 [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661) 논문을 정리해보고자 합니다.

논문의 파트 별로 요약하며 개념이 부족할 수 있는 단어나 수식을 추가 설명하는 식으로 정리해보았습니다. 추가되었으면 하는 부분이나 이해가 가지 않으시는 부분은 댓글로 남겨주시면 저도 아직 부족하지만 최대한 답을 해드릴 수 있도록 노력해보겠습니다. 잘못된 부분을 지적해주것도 언제나 환영합니다. 감사합니다:)

## 1. Introduction
지금까지의 딥러닝 모델들 중 가장 눈에 띄는 성공적인 모델은 다양한 입력을 클래스 라벨로 분류하는 고차원의 판별 모델(discriminative model)과 관련되어 있다. 이러한 놀라운 성공은 gradient 학습에 특히 잘 작동하는 piecewise linear units를 사용하여 주로 dropout과 역전파 알고리즘을 기반으로 한다. 심층 생성 모델은 최대 우도 측정(maximum likelihood estimation)을 사용하는데 여기서 발생하는 확률론적 계산을 근사화하는 것이 어렵고 PLU의 장점을 생성할 때 활용하기 어렵기 때문에 눈에 띄지 않는 모델이였다. 우리는 이러한 어려움을 회피하는 새로운 생성 모델 추정 절차를 제안한다.

<details>
<summary>PLU(piecewise linear units)</summary>
  범위 별로 나뉘어진(piecewise) 직선 함수가(linear) activation 함수로 쓰이는 경우 이 함수를 piecewise linear units라 합니다.  
  <br>
  아래의 그림처럼 범위가 나뉘어져 $ x $ 값에 따라 적용되는 직선의 함수가 다른 경우 piecewise linear functio라 하며 이 함수를 activation 함수로 사용한다면 piecewise linear unit을 사용했다 할 수 있습니다.

  <br><br>
  <div style="float:left;margin:0 10px 10px 0" markdown="1">
    <img src="/assets/images/posts/gan/plu_expression.png" width="250" height="120">
  </div>
  <div style="margin:0 10px 10px 0" markdown="1">
    <img src="/assets/images/posts/gan/plu_graph.png" width="170" height="120">
  </div>
</details>

<details>
<summary>최대 우도 측정 /(MLE / maximum likelihood estimation)</summary>

</details>

<br>
제안하는 적대적 네트워크 프레임워크에서 생성모델은 판별모델과 대립된다. 결과 이미지가 생성 모델 분포에서 왔는지 학습 데이터 분포에서 왔는지를 결정하는 것을 판별 모델이 학습한다. 생성 모델을 위조 지폐를 제작해 적발 없이 사용하려는 위조 지폐 팀과 유사하다고 생각할 수 있고 판별 모델은 위조 지폐를 적발하려는 경찰과 유사하다고 볼 수 있다. 이 게임의 경쟁은 위조품과 진짜 물건을 구분할 수 없을 때까지 두 팀 모두 방법을 개선하도록 만든다.

본 논문에서 생성 모델과 판별 모델은 다층 퍼셉트론으로 이루어져 있으며 이 특수한 케이스를 적대적 네트워크(Adversarial nets)라 부른다. 우리는 두 모델을 역전파, dropout 알고리즘을 사용하려 훈련하고 순전파만을 사용하여 생성 모델에서 결과 이미지를 추출할 수 있다. approximate inference나 Markov chains는 필요하지 않다.

<details>
<summary>approximate inference</summary>
  보통 inference는 모델의 학습이 끝난 후 모델에게 데이터를 입력해 결과를 추론하는 과정을 말하지만 approximate inference는 모델을 학습하는 과정에서 수행되는 inference 과정을 말합니다. 이때 approximate inference는 사후확률(posterior) 분포($p(z|x)$) 다루기 위해 사용되며 크게  Sampling Method와 Variational Method로 구분됩니다.

  <br><br>
  논문에서 approximate inference를 언급한 이유는 GAN 이전에 사용되던 대표적인 생성모델인 VAE(Variational Auto-Encoder)가 approximate inference 방법 중 하나인 variational inference(변분추론)를 사용하기 때문입니다. VAE에서는 decoder가 데이터의 사후확률을 학습하는데 사후확률은 계산이 복잡한 경우가 많아 이를 다루기 쉬운 분포 $q(z)$로 근사할 때 vational inference를 사용합니다.

  <br><br>
  참고
  - Kim Hyungjun 님의
  <a href="https://kim-hjun.medium.com/approximate-inference%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80-35653b963546">Approximate Inference란 무엇인가?</a>
  - ratsgo 님의 <a href="https://ratsgo.github.io/generative%20model/2017/12/19/vi/">변분추론(Variational Inference)</a>
</details>

<details>
<summary>Markov chains</summary>
  특정 상태의 확률이 오직 과거의 상태에 의존하는 성질을 Markov 성질이라 합니다. Markov chain이란 이런 Markov 설징을 가진 이산 확률 과정을 의미합니다.

  대표적인 예시로 학생 때 어디선가 많이 본 듯한 문제인 비가 올 확률을 구하는 문제입니다. "비가 온 다음 날에 비가 올 확률은 70%이고 비가 오지 않은 다음 날에 비가 올 확률은 20%입니다" 라는 문장을 모델화 한다면 다음과 같은 그림이 나오게 됩니다.
  "이미지"

  "확률"
  위와 같은 상태 전이도를 가지고 있을 때 전이 행렬은 P와 같으며 이런 전이 행렬을 가지고 계산하는 것이 Markov chain입니다. 예시로 비가 온 날의 3일 후 비가 올 확률을 계산해보겠습니다.



  참고
  -

</details>


## 2. Related Work



## 3. Adversarial nets



## 4. Theorical Results



## 5. Experiments



## 6. Advantages and disadventages



## 7. Conclusions and future work
