---
layout: post
title: DCGAN
subtitle : Unsupervised Representation Learning With Deep Convolutional Generative Adversarial Networks
# subtitle:
categories: gan
tags: [dcgan, gan, 생성 모델, 논문 구현]
# sidebar: []
use_math: true
---

## 0. 소개
드디어 CNN 계열의 생성 모델로 들어왔습니다! convolution과 transposed convolution을 사용한 것이 특징인 <a href="https://arxiv.org/abs/1511.06434v2" target="_blank">DCGAN</a>을 소개합니다.

classification과 같은 지도학습 task에서 좋은 성능을 보이던 CNN을 비지도 학습 분야인 이미지 생성 task로 가져와 사용한 것이 특징으로 모델의 기능을 증명하기 위한 다양한 실험을 진행했습니다.<br>
Discriminator의 경우 feature extractor로 사용되어 얼마만큼의 성능을 내는지에 대한 실험, Generator의 경우 데이터 셋을 기억하지 않고 생성함에 대한 증명, latent space의 변화 과정, 산술 연산 과정의 실험을 진행하여 생성된 이미지의 시각적인 부분에 대한 증명이 아닌 수치적인 증명까지 자세하게 나와있어 실험에 대한 진심을 느낄 수 있었던 논문입니다.<br>

저는 <a href="https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html" target="_blank">CelebA</a> 데이터셋을 사용해 얼굴 생성에 대한 DCGAN을 구현을 해 관련 실험 부분은 코드와 함께 논문을 설명해드리고 그 외 데이터셋과 실험은 논문 내용을 요약해 설명해드리겠습니다.

## 1.  데이터 셋

### 1.1. 논문 데이터셋
논문에서는 총 3가지의 데이터 셋을 사용했습니다. <a href="https://www.yf.io/p/lsun" target="_blank">LSUN</a>, Face, <a href="https://image-net.org/download.php" target="_blank">ImageNet-1k</a>입니다.

<a href="https://www.yf.io/p/lsun" target="_blank">LSUN</a>은 침실 데이터 셋을 이용했으며 생성 모델이 이미지를 기억해 생성하는 memorization 가능성을 줄이기 위해 이미지 중복 제거 프로세스를 진행했습니다. 이미지를 32x32로 downsampling한 후 3072-128-3072 Denoising AutoEncoder(DAE)와 ReLU activation을 사용했다고 합니다.

---
그림
---

위의 그림과 같은 방식을 사용했다 이해했습니다.
32x32 이미지를 입력으로 넣어 128의 latent vector를 DAE를 이용해 추출한 후 이 벡터를 ReLU를 통해 이진화함으로써 32 bit의 이진화된 코드를 가지게 됩니다.
32 bit의 이진화된 코드는 해시 값이 됩니다. 해시 충돌이 일어난 이미지는 중복 이미지로 처리되게 됩니다. 이 이미지들을 육안으로 검사했을 때 false positive 확률이 1/100 미만이였을 정도로 높은 정밀도를 보여주었다고 하며 거의 275,000개의 이미지를 삭제했으며 높은 recall을 보여주었다고 합니다.

중복 이미지를 삭제하는 방법으로 이미지를 latent vector로 만들어 해시로 사용한다는 부분이 신선했던 부분입니다. 기존 LSUN 침실 데이터 셋의 크기가 3,033,042개로 약 300만개 정도인데 275,000개의 이미지를 삭제한 것은 삭제한 양이 결코 작지 않음에도 해싱 방법으로 효율적으로 처리했다는 것이 대단하다 느껴졌습니다.

Face 데이터의 경우 웹 이미지 쿼리를 통해 현대 사람 얼굴을 포함한 이미지를 추출해 10,000명의 사람들의 300만개의 이미지를 얻었다고 합니다. OpenCV를 이용해 이미지에서 얼굴을 검출해 높은 해상도를 유지한 350,000개의 얼굴이 있는 이미지를 얻었으며 추가적인 augmentation은 적용하지 않은 채 사용했다 합니다.

Imagenet-1k는 32x32의 center-crop한 이미지를 사용했으며 마찬가지로 augmentation은 적용하지 않았다 합니다.

### 1.2. 구현 데이터셋
LSUN의 데이터는 용량이 커서 그런지 다운 중 서버 응답이 끊기는 경우가 계속 발생해 다운에 실패했습니다...(._.  )<br>
대신 Face 데이터를 사용하기로 했는데 웹 쿼리를 사용할 필요 없이 간단하게 사용할 수 있는 <a href="https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html" target="_blank">CelebA</a>를 사용했습니다.<br>
논문에서는 약 350,000개의 이미지를 사용했다 되어있으며 CelebA는 202,599개의 이미지를 가지고 있습니다. 장 수가 부족하지만 논문에서도 augmentation 작업이 없었다 언급되어 있어 augmentation 작업을 추가하지 않았습니다.

```python
# img_size = 64
dataset = ImageFolder('E:\\DATASET\\celeba',
                      transform=transforms.Compose([transforms.Resize((img_size, img_size)),
                                                    transforms.ToTensor()]))
dataloader = DataLoader(dataset=dataset,
                        batch_size=batch_size,
                        shuffle=True,
                        drop_last=True)
```
CelebA에서 Align&Cropped Images를 다운받아 사용했습니다. 논문에서는 generator의 결과로 64 x 64 의 이미지를 생성한다 했으니 데이터셋의 이미지 크기도 같게 만들어주기 위해 transforms를 이용해 resize를 해주었습니다.


<img src="/assets/images/posts/dcgan/celeba.png" width="600" height="600">
```python
images, labels = next(iter(dataloader))

plt.figure(figsize=(12, 12))
plt.title("CelebA images")
plt.imshow(make_grid(images[:64, :, :, :]).permute(1, 2, 0))
plt.axis('off')
plt.savefig('./result/celeba.png', dpi=300)
plt.show()
```
데이터 셋을 시각화해보았습니다. 얼굴만 딱! 잘 나오는 것을 확인할 수 있습니다.<br>
왠지 이병헌님처럼 보이는 분이 있는데 20만장이 넘는 이미지 중에서 64장 시각화한 결과에서 보이니 괜시리 반갑고 그러네요ㅎ


## 2. 모델
모델 구조에 대해서는 논문에 확실하게 언급된 부분들이 꽤나 많았습니다.
- maxpooing과 같은 spatial pooling 함수를 strided convolutions(discriminator)와 fractionally strided convolutions(generator)로 대체한다.
- convolution 이후 fully connected layer를 제거한다.
- discriminator의 경우 마지막 convolution 레이어가 flatten된 다음 sigmoid 출력으로 이어져 결과를 출력한다.
- batchnorm를 discriminator와 generator의 모든 레이어에 적용하되 generator의 출력 레이어와 discriminator의 입력 레이어에는 적용하지 않는다.
- ReLU는 generator의 출력 레이어인 Tanh를 제외한 모든 레이어에 사용한다.
- LeakyReLU는 discriminator의 모든 레이어에 사용한다.

<br>
모델 구조 외에도 아래와 같은 디테일한 점들이 함께 논문에 언급되어 있습니다.
- 128의 mini batch 크기를 가진 mini batch stochastic gradient descent(SGD)로 학습한다.
- std가 0.02로 zero-centered된 Normal distribution으로 모델 가중치를 초기화한다.
- Adam을 사용하여 0.001의 학습률은 너무 값이 크니 0.0002를 사용하며 momentum $\beta_1$는 0.9가 아닌 0.5로 사용한다.
- Z는 100차원의 uniform 분포이다.

언급된 사항들을 모두 포함해 모델을 구현해봅시다!

### 2.1 fractionally strided Convolution
모델 구현 전 convolution에 대해 짚고 넘어가겠습니다.
흔히 사용하는 Convolution / Conv2d는 strided convolution으로 여기서는 discriminator에 사용됩니다..

---
그림
---
filter가 input 위를 window 처럼 훑으며 지나가는 모습이고 back propagation으로 filter의 weight 및 bias가 업데이트됩니다.

fractionally strided convolution은 transposed convolution / ConvTranspose2d로 여기서는 generator에 사용됩니다.
논문에서 이 transpose convolution을 몇몇은 deconvolution이라 잘 못 부른다 언급되어 있는데 deconvolution은 convolution의 연산 결과를 되돌리는 역함수와 같은 연산을 말하며 transpose convolution은 deconvolution과는 다르다는 것을 알아야 합니다.

---
그림
---
저는 사실 이 그림만 봐서는 이해를 하지 못했습니다.... 그래서 다시 그렸습니다! 제가 이해한 방식으로 다시 그려보았습니다ㅎ-ㅎ

---
그림
---

transposed convolution은 ~~~~


<img src="/assets/images/posts/dcgan/convtranpose2d.png" width="600" height="200">
같은 내용이 pytorch의 설명에도 나와있는 걸 보니 과거에 혼동해 쓰는 경우가 많았었나 봐요...

### 2.2. weight initialize
모델의 모든 가중치는 표준편차 0.02로 zero-centered된 Normal distribution으로 초기화했다 언급이 되어있습니다.<br>
`nn.init.normal_`로 convolution 모듈의 가중치를 mean=0.0, std=0.02 값으로 설정해 가중치 값을 초기화해주었습니다.

```python
# 0.02 표준 편차로 zero-ceneterd normal distribution으로 initialize
def init_weight(module):
    if type(module) in [nn.Conv2d, nn.ConvTranspose2d]:
        nn.init.normal_(module.weight.data, mean=0.0, std=0.02)
```

### 2.3. Generator
<img src="/assets/images/posts/dcgan/fig1.png" width="500" height="250">

### 2.4. Discriminator



## 3. 생성 결과


### 3.1. 필터 시각화


### 3.2. 보간 방법


## 4. 논문의 추가 실험

### 4.1. feature extract

### 4.2. (논문)이미지 산술 연산

### 4.3. (논문)객체 삭제
