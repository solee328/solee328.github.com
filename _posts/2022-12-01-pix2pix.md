---
layout: post
title: Pix2Pix(1) - 논문 리뷰
# subtitle:
categories: gan
tags: [gan, pix2pix, 생성 모델, 논문 리뷰]
# sidebar: []
use_math: true
---

## 소개

<img src="/assets/images/posts/pix2pix/paper/fig1.png" width="600" height="270">
Pair 데이터를 사용한 생성 모델인 Pix2Pix

논문 모든 내용을 번역하지 않고 중요한 부분들을 요약해 번역 -> 의역 존재


## Abstract
image-to-image 변환 문제에 대한 범용 솔루션으로 conditional adversarial networks(이후 조건부 GAN으로 대체 사용하겠습니다)를 사용하며 입력 이미지에서 출력 이미지로의 매핑을 학습하기 위한 loss 함수를 사용합니다. label map, edge map에서 개체를 합성 / 재구성하고 이미지를 colorizing에 효과적이라는 것을 보여줍니다.

## 1. Introduction
이미지 처리, 컴퓨터 그래픽스, 컴퓨터 비전의 많은 과제들은 입력 이미지를 출력 이미지로 변환하는 것에 대해 질문을 던집니다. 장면(이미지)는 RGB image, gradient field, edge map, semantic label map 등 다양하게 렌더링될 수 있는데 자동 언어 번역과 유사한 자동 image-to-image 변환을 충분한 학습 데이터가 주어진다면 한 장면의 표현을 다른 장면으로 변환하는 작업으로 정의할 수 있습니다. 과거의 이런 과제들은 각각 별도의 특수 목적의 시스템으로 처리되었지만 본 논문에서 우리의 목표는 이러한 모든 문제에 대한 공통된 프레임워크를 개발하는 것입니다.

convolutional neural nets(CNNs)로 다양한 이미지 예측 문제를 해결하는 방향으로 연구들이 진행되고 있고 CNNs는 loss 함수를 최소화하는 방향으로 학습합니다. 하지만 효과적인 loss 함수를 만드는 것은 아직 많은 수작업이 필요합니다. 단순한 접근 방식으로 CNN에게 예측(predicted)과 실제(ground truth)의 유클리디안 거리를 최소화하는 것은 흐릿한 결과를 생성하는 경향이 있습니다. 유클리디안 거리를 최소화하는 것은 결과의 모든 값을 평균화하는 것이기 때문이 흐릿한 결과를 유도하기 때문입니다.

'현실과 구별할 수 없는 출력을 만든다'와 같이 고수준의 목표를 명시하는 대신 목표를 달성하기 위한 적절한 loss 함수를 학습할 수 있다면 매우 바람직합니다. 다행히 최근 제안된 Generative Adversarial Networks(GANs)에 의해 수행할 수 있으며 GANs는 출력이 실제인지 가짜인지 구별하는 판별 모델 loss를 학습하는 동시에 이 loss를 최소화하기 위한 생성 모델 또한 학습합니다. 흐릿한 이미지는 명백히 가짜로 보이기 때문에 GANs는 흐릿한 이미지를 가짜로 판별하고 생성 모델은 더 선명한 이미지를 생성하게 될 것입니다.

본 논문에서 우리는 GANs의 조건 설정을 탐구합니다. GAN이 데이터 생성 모델을 학습하는 것처럼 조건부 GANs(cGAN)은 조건부 생성 모델을 학습합니다. cGAN은 조건부 입력 이미지에 따라 출력 이미지를 생성하는 image-to-image 변환에 적합합니다.

조건부 GANs가 다양한 과제들에 대해 합리적인 결과를 생성한다는 것을 입증합니다. 코드는  https://github.com/phillipi/pix2pix 에서 확인할 수 있습니다.

## 2. Related work
Structed losses for image modeling, Conditional GANs와 관련된 논문들이 열거되어 있습니다.
Structed losses for image modeling에서 image-to-image 변환 문제는 주로 per-pixel classification이나 regression으로 수식화하고 문제를 해결했으나 조건부 GAN은 loss를 학습한다는 점에서 기존 수식 모델과는 다르고 출력과 정답의 오차가 있는 구조에 대해 loss를 이용해 불이익을 줄 수 있다는 점이 나와있습니다.

Conditional GANs에서는 조건부 Gan이 inpainting, future state prediction, super resolution과 같은 여러 image-to-image 분야에서 인상적인 결과를 달성한 분야들이 있었습니다. 각 방법들은 목적에 맞는 다른 term(예시: L2 regression)들로 loss를 조정했는데 본 논문의 연구는 특정 목적이 없는 것이 특징입니다. 또한 이전 논문들과는 생성 모델과 판별 모델의 구조에 차이가 있는데 생성 모델은 'U-Net'을 사용하고 판별 모델은 'PatchGAN'을 사용하는 것이 특징입니다.

## 3. Method

<img src="/assets/images/posts/pix2pix/paper/fig2.png" width="400" height="200">

GANs는 랜덤 노이즈 벡터 $z$를 출력 이미지 $y$로 매핑하는 매핑 $G : z \rightarrow y$ 을 수행하는 모델입니다. 대조적으로 조건부 GANs는 조건에 해당하는 이미지 $x$와 랜덤 노이즈 벡터 $z$를 출력 이미지 $y$로 매핑하는 매핑 $G : \{ x, z \} \rightarrow y$ 을 학습해 수행합니다.

### 3.1. Objective
조건부 GAN의 목적 함수는 아래의 수식과 같습니다.

$$
\mathcal{L} _{cGAN}(G, D) = \mathbb{E} _{x,y} [logD(x, y)] + \mathbb{E} _{x, z}[log(1-D(x, G(x, z)))] \tag{1}
$$

$G$는 목적 함수를 최소화하려고 하며 목적 함수를 최대화하려는 $D$에 대항합니다. 이를 다시 정리하면 $G^* = arg \min_G \max_D \mathcal{L} _{cGAN}(G, D)$ 로 표현할 수 있습니다.

[전통적 목적 함수를 섞으면 도움이 되며 L2보단 L1이 좋았음]
<a href=https://arxiv.org/abs/1604.07379 target='_blank'>'Context Encoders: Feature Learning by Inpainting'</a>과 같은 연구는 전통적인 loss와 GAN의 목적 함수를 섞는 것이 도움이 된다는 것을 발견했습니다. 판별 모델이 생성 모델이 생성한 가짜 이미지와 진짜 이미지를 구별해야 한다는 목적은 언제나 같지만 생성 모델의 경우 판별 모델을 속일 뿐만 아니라 실제 이미지와 최대한 유사해야 하기 때문에 L2 loss가 도움이 됩니다. 저자들은 L1이 흐릿함을 덜 유발한다는 점에서 L1 distance가 L2 distance 보다 사용하기 좋다는 것 또한 연구했다 합니다.

$$
\mathcal{L}_{L1}(G) = \mathbb{E} _{x, y, z}[\| y-G(x, z) \|_1]
\tag{3}
$$

위의 두 식을 합쳐 본 논문에서 사용하는 최종 목적 함수는 다음와 같습니다.

$$
G^* = arg \min\limits_{G} \max\limits_{D} \mathcal{L} _{cGAN}(G, D) + \lambda\mathcal{L} _{L1}(G)
\tag{4}
$$

[z 입력을 주지 않고 dropout으로 해결하고자 했으나 실패]
$z$를 입력으로 같이 넣어주지 않는다해도 $x$를 $y$로 매핑하는 것을 모델이 학습할 수 있지만 확정된 출력 값을 생성하며 delta function 의 분포만 포현할 수 있습니다. delta function은 --- 입니다. Mathieu --- 연구의 경우 --로 노이즈 $z$가 필요하지 않았으며 $z$를 넣어 학습해도 $z$를 무시하는 방향으로 학습했습니다. 따라서 본 연구에서는 노이즈 $z$를 $G$에 입력하지 않으며 대신 dropout을 이용했습니다. dropout으로 노이즈를 제공했지만 결과에서는 minor stochasticity만이 발견되었으며 이러한 low stochasticity는 현재 연구에서 해결되지 않는 중요한 문제 입니다.


### 3.2. Network architectures
[44]dcgan의 생성 모델과 판별 모델 구조를 채택했습니다. 생성 모델과 판별 모델 모두 convolution-BatchNorm-ReLu[29] 구조의 모듈을 사용합니다.

#### 3.2.1 Generator with skips
<img src="/assets/images/posts/pix2pix/paper/fig3.png" width="500" height="250">

[encoder-decoder에 skip connection을 추가한 unet 사용]
image-to-image 변환 문제는 고해상도 입력을 고해상도 출력으로 매핑해야 한다는 것입니다. 입출력 이미지의 텍스쳐 등의 질감을 달라야 하지만 기본 구조는 같도록 만들어야 한다. 이 문제에 대한 이전의 다양한 솔루션 [43, 55, 30, 64, 59]는 encoder-decoder 네트워크[26]를 사용합니다. 이 네트워크는 bottle neck 이라 불리는 레이어까지 downsample되는 레이어들을 통과합니다. 이미지 변환 문제의 경우 입력과 출력 사이에 low-level 정보들이 공유되고 이 정보들은 네트워크를 통해 전달되는 것이 바람직합니다. 하지만 bottle neck으로 이런 정보들이 손실되는 것을 피하기 위해 'U-Net'의 모양을 따라 skip connection을 추가합니다. 우리는 각 레이어 $i$와 레이어 $n - i$ 사이에 skip connection을 추가하며 여기서 $n$은 전체 레이어의 수이다.

-> 추가설명(low level informatino / unet skip connection 의미(bottle neck이 왜 정보가 손실되는가 skip connection으로 channel 변경은 어찌 되는가))
여기서 low-level 정보는 이미지의 low level feature를 말하며 이미지의 edge, corner, color 등이 해당됩니다.

#### 3.2.2 Markovian discriminator (PatchGAN)
[patch gan 사용 이유]
L2, L1 loss가 이미지 생성 문제에 대해 흐릿함(blur)을 생성한다는 것은 잘 알려져 있습니다[34]. L2, L1 loss는 high frequency를 잘 포착하지 못하지만 low frequency를 정확하게 포착합니다. 따라서 low frequency 정확도를 높이기 위해 L1 term을 사용하며 이 L1이 high frequency 정확도가 낮다는 것을 알기에 판별 모델을 high frequency 구조만 모델링하도록 제한할 동기를 부여한다. 로컬 이미지 패치의 구조에 대해 집중하는 것으로 고주파를 모델링할 수 있기에 패치 규모로만 수행되는 판별 모델 구조를 설계합니다. 이 판별 모델은 N x N 패치가 실제인지 가짜인지 분류하며 모든 응답의 평균을 $D$의 출력으로 내보냅니다.

4.4에서 patch의 크기가 이미지의 전체 크기보다 훨씬 작은 경우에도 고품질의 결과를 생성할 수 있음을 보여줍니다. 작은 PatchGAN은 파라미터 수가 작고 속도가 빠르며 임의의 큰 이미지에도 적용할 수 있다는 장점이 있습니다.

이런 판별 모델은 지정된 patch 크기보다 멀리 있는 픽셀 간은 독립되어 있다 가정하며 Markov random field 에 효과적입니다. 텍스처[17, 21], 스타일[16, 25, 22, 37]에서 연구된 적이 있으므로 patchGAN 텍스처 / 스타일 loss로 이해할 수 있습니다.

### 3.3. Optimization and inference
[24]의 접근법을 따르며 $D$와 $G$를 번갈아 가며 gradient descent를 수행합니다. 기존 GAN 논문에서 제안된 것처럼 $log(1-D(x, (G(x, z))))$를 최소화하도록 $G$를 학습하는 대신 $log(x, G(x, z))$를 최대화하도록 학습합니다. 또한 $D$의 학습단계에서 목적 함수(=loss)를 2로 나누어 $D$의 학습 속도를 늦춥니다. 학습에서는 mini batch SGD를 사용하고 learning rate가 0.0002이고 momentum의 파라미터가 $\beta_1=0.5$, $\beta_2=0.999$인 Adam solver[32]를 사용합니다.

inference 시 학습 단계와 정확히 같은 방식으로 생성 모델 네트워크를 실행한다. 즉 test 때에도 dropout을 적용하며 training batch가 아닌 test batch를 사용해 batch normalization을 적용한다는 점에서 일반적인 프로토콜과는 다릅니다. batch 크기를 1로 설정할 때 batch normalization을 'instance normalization'이라 하며 이미지 생성에서 효과적인 것으로 [54]에서 입증되었습니다. 본 논문에서는 실험에 따라 batch 크기가 1에서 10 사이를 사용합니다.


## 4. Experiments
조건부 GAN의 일반성을 탐구하기 위해 사진 생성과 같은 그래픽 작업과 semantic segmentation 과 같은 비전 작은을 포함한 다양한 작업과 데이터 셋에서 테스트를 진행했습니다.
- semantic labels $\leftrightarrow$ photo
- architectural $\leftrightarrow$ photo
- map $\rightarrow$ aerial photo
- BW $\rightarrow$ color photos
- Edges $\rightarrow$ photo
- sketch $\rightarrow$ photo
- day $\rightarrow$ night
- thermal $\rightarrow$ color
- photo with missing pixels $\rightarrow$ inpainted photo


$$
이미지, figure 14, 15
$$

작은 데이터 셋에서도 종종 괜찮은 결과를 얻을 수 있었다 합니다. facade 학습 셋은 400 개의 이미지로만 구성되어 있으며 day $rightarrow$ night 학습 셋은 91 개의 웹캠 이미지로만 구성되어 있음에도 위와 같은 결과를 얻었습니다.

### 4.1. Evaluation metrics
생성된 이미지의 품질을 평가하는 것은 미해결된 어려운 문제입니다. pixel 당 mean-squared error와 같은 전통적인 메트릭은 결과의 구조물들을 통계나 구조를 고려하지 않기 때문에 한계가 존재합니다.

결과를 전체적으로 평가하기 위해 2가지 전략을 사용합니다. 첫번째는 Amazon Mechanical Turk(AMT)로 'real vs fake'에 대한 사람이 평가하는 인식테스트입니다. 두번째는 'FCN-score'로 기존의 분류 모델을 합성된 이미지 안의 물체를 인식할 수 있을 정도로 합성된 이미지가 현실적인지 여부를 측정하는 방법입니다.

**AMT perceptual studies**
AMT의 경우 [62]의 방법을 따릅니다. Turker는 작업자를 의미합니다. Turker는 알고리즘에 의해 생성된 'fake' 이미지와 'real' 이미지를 비교하는 실험을 진행합니다. 각 실험에서 이미지는 1초 동안 보여진 후 turker들은 어떤 것이 가짜인지에 대해 판단할 수 있는 시간이 주어집니다. 각 세션마다 처음 10개 이미지는 연습이며 연습마다 turker들은 피드백을 받아 정답을 학습하게 됩니다. 이후 40개의 이미지가 주어지며 이에 대한 피드백은 제공되지 않습니다. 50명 이하의 turker들이 알고리즘을 평가하며 [62]와는 다르게 vigilance trials를 포함하지 않았습니다.

**"FCN score"**
생성 모델의 정량적 평가는 어려운 것으로 알려져 있지만 최근 연구 [52, 55, 62, 42]는 생성 이미지들 판별 가능성을 측정하고자 유사 메트릭으로 사전 학습된 semantic classifier를 사용했습니다. 생성된 이미지가 현실적이라면 실제 이미지에 대해 학습된 판별 모델도 생성된 이미지를 정확하게 분류할 수 있다는 것이 아이디어입니다. 이를 위해 semantic segmentation으로 인기 있는 FCN-8s[39] 구조를 채택해 도시 경관 데이터 셋(데이터 셋 설명)에서 학습시킨 후 생성된 이미지에 대한 분류 정확도로 생성된 사진의 점수를 측정합니다.


### 4.2. Analysis of the objective function
ablation study(ablation study 정리)를 실행해 L1 term, GAN term의 영향을 분리하고 조건부 판별 모델(cGAN, Eqn. 1), 조건 없는 판별 모델(GAN, Eqn. 2)를 사용하는 것과 비교합니다.

<img src="/assets/images/posts/pix2pix/paper/fig4.png" width="600" height="400">

Figure 4는 labels $\rightarrow$ photo 문제에 대해 비교에 대한 질적 효과를 보여줍니다. L1 만으로도 합리적인 결과긴 하지만 흐릿함을 볼 수 있습니다. 조건부 GAN 단독(Eqn. 4에서 $\lambda=0$으로 설정) 사용은 훨씬 더 sharp한 결과를 제공하지만 시각적 아티팩트(노이즈)가 발견됩니다. 두 term을 모두 추가하면($\lambda=100$) 아티팩트가 줄어듭니다.

<img src="/assets/images/posts/pix2pix/paper/table1.png" width="300" height="150">

Table 1은 Cityscape $\leftrightarrow$ photo의 FCN 점수를 사용해 정량화한 것입니다. 조건부 GAN인 cGAN은 L1에 비해 더 높은 점수를 달성했으며 생성 이미지에 모델이 인식 가능한 구조가 더 있음을 의미하며 더 현실적인 이미지를 생성했다 할 수 있습니다. 조건부가 없는 GAN의 경우 결과를 조사하면 입력 사진과 상관없이 거의 동일한 출력을 생성하는 model collapsed 가 생성 모델에 발생했음을 알 수 있었다 합니다. L1 term을 추가하면 L1 loss가 출력이 정답과의 거리를 비교해 loss에 더하기 때문에 더 좋은 결과가 나옴을 볼 수 있습니다.


**colorfulness**

<img src="/assets/images/posts/pix2pix/paper/fig7.png" width="550" height="200">

L은 명도를 나타내며 a, b는 ...?

L1이 edge를 정확히 어디에 위치시킬지 불확실할 때 흐릿함을 발생시키는 것과 같이 픽셀이 어떤 색을 취해야하는지 불확실할 때 평균적인 회색을 띄는 색을 발생시킵니다. term을 최소화하기 위해 확률 밀도 함수의 중위수를 선택하게 되고 이 수치는 회색에 가까운 색이기 때문입니다. 반면 adversarial loss는 회색 출력이 비현실적이라는 것은 판별 모델이 인식하게 되니 실제 색상 분포와 일치하는 방향으로 학습할 수 있습니다. Figure 7을 통해 색상에 대한 loss의 효과를 볼 수 있습니다. L1이 ground truth보다 더 좁은 분포로 이어지며 이는 L1이 평균적인 회색빛을 조장한다는 가설을 확인시켜줍니다. 반면 cGAN은 출력 분포를 실제 값에 더 가깝게 생성합니다.

### 4.3. Analysis of the generator architecture
$$
figure 5, table 2 그림 추가
$$

Figure 5와 Table2로 U-Net과 encoder-decoder를 비교해 skip connection의 효과를 볼 수 있습니다. L1 loss로만 학습할 때, L1 + cGAN loss로 학습할 때 모두 U-Net이 더 우수한 결과를 나타냄을 볼 수 있습니다.

### 4.4. From PixelGANs to PatchGANs to ImageGANs
$$
figure 6, table 3 그림 추가
$$

판별 모델의 receptive fields인 patch 크기 N을 1 x 1 'PixelGAN'에서 286 x 286 'ImageGAN'으로 변경했을 때의 효과를 테스트했습니다. Figure 6은 이 테스트의 결과를 보여주며 Table 3은 FCN 점수를 사용해 결과를 정량화합니다. 본 논민이 다른 곳에서 명시되지 않는 한 모든 실험이 70 x 70 PatchGAN을 사용했으며 이 섹션에서는 모든 실험이 L1 + cGAN loss를 사용했다는 것에 유의해야 합니다.

PixelGAN은 spatial sharpness에는 영향이 없지만 색상의 다양성을 증가시킵니다(Figure 7). 예시로 Figure 6의 버스는 네트워크가 L1 loss 였을 때 회색으로 칠해지지만 PixelGAN loss 에서는 빨간색이 된 것을 볼 수 있습니다.

16 x 16 PatchGAN을 사용하면 sharp한 출력이 가능하고 좋은 FCN 점수를 달성할 수 있지만 tiling 아티팩트가 나오기도 합니다. tiling 아티팩트란 ~~~~. 70 x 70 PatchGAN을 사용하면 tiling 아티팩트를 완화하고 조금 더 나은 점수를 달성합니다. 286 x 286 ImageGAN은 결과의 시각적 품질을 향상시키지 않는 것으로 보이며 실제로 FCN 점수가 상당히 낮은 것을 Table 3에서 확인할 수 있습니다.



<img src="/assets/images/posts/pix2pix/paper/fig8.png" width="550" height="300">

**Fully-convolutional translation**
PatchGAN의 장점은 고정 크기 patch 판별 모델을 임의의 큰 이미지에 적용할 수 있다는 것입니다. 생성 모델 또한 학습에 사용한 이미지보다 더 큰 이미지에 convolution처럼 적용할 수 있습니다. 우리는 map $\leftrightarrow$ aerial 작업에서 테스트했습니다. 256 x 256 이미지에 생성 모델을 학습할 후 512 x 512 이미지에 대해 테스트했으며 Figure 8을 통해 확인할 수 있습니다.

### 4.5. Perceptual validation

<img src="/assets/images/posts/pix2pix/paper/table4.png" width="400" height="150">

map $\leftrightarrow$ aerial 과 grayscale $\leftrightarrow$ color 작업에 대한 사실성을 검증합니다. map $\leftrightarrow$ photo에 대한 AMT 실험 결과는 Table 4를 통해 확인할 수 있습니다. L1 + cGAN loss로 생성된 사진은 L1 baseline보다 훨씬 더 높은 18.9%의 실험자들을 속였으며 L1은 흐릿한 결과를 생성해 실험자들을 거의 속이지 못했습니다. 대조적으로 photo $\rightarrow$ map 에서 L1 + cGAN loss로 생성한 이미지는 6.1% 실험자들을 속였으며 L1 basline 성능과 크게 다르지 않았다. 이는 사소한 구조의 오류가 실제 항공 사진보다 기하학 구조를 가진 지도에서 더 잘 보이기 때문일 수 있습니다.


$$
figure 9 Table 5, 그림 추가
$$

ImageNet에서 colorization [62, 35]에서 도입한 테스트 분할에 대해 실험을 진행했습니다. L1 + cGAN loss로 만든 이미지는 22.5%의 실험자들을 속였음을 Table 5에서 확인할 수 있습니다. [62]의 결과와 L2 regression을 함께 실험했습니다. cGAN과 L2 regression은 [62]과 유사한 점수를 얻었지만 27.8% 실험자를 속인 [62]에는 미치지 못했습니다. [62]가 colorization을 수행하도록 특별히 설계된 구조이며 pix2pix는 범용적인 구조라는 것에 주목해야 합니다.

### 4.6. Semantic segmentation

$$
Figure 10, Table 6, 그림 추가
$$

조건부 GAN은 이미지 처리 및 그래픽 작업에서 출력이 상세한 현실적인 과제들에 효과적으로 보입니다. 그렇다면 출력이 입력보다 덜 복잡한 semantic segmentation과 같은 문제에서도 효과적일까요?

이 실험을 시작하기 전 cityscape photo $\rightarrow$ label 데이터 셋으로 cGAN(L1 loss가 있는 경우 / 없는 경우)을 학습시킵니다. Figure 10은 이에 대한 질적 결과를 보여주며 정량적 분류 정확도는 Table 6을 통해 확인할 수 있습니다. 흥미롭게도 L1 loss 없이 학습된 cGAN으로도 합리적인 정확도로 문제를 해결할 수 있음을 보였습니다. 기존 segmentation 방법과 달리 cGAN으로 라벨을 생성하는 semantic segmentation을 성공적으로 생성한 첫 번째 시도입니다. cGAN 만으로 학습한 것보다 L1 + cGAN이 조금 더 나은 결과를 보이지만 단순히 L1 regression을 사용한 방법이 가장 높은 정확도를 달성했습니다.

### 4.7. Community-driven Research

$$
figure 12, figure 11 그림 추가
$$

논문과 pix2pix 코드 베이스를 최초 공개한 이후, 컴퓨터 비전과 그래픽 실무자 뿐만 아니라 비주얼 아티스트를 포함한 트위터 커뮤니티는 논문의 범위를 훨씬 벗어나 다양하고 새로운 image-to-image 변환 작업에 프레임워크를 성공적으로 적용했습니다. Figure 11과 Figure 12는 배경 제거, 팔레트 생성, 스케치 $\rightarrow$ 초상화, 포즈 전송, #edges2cats 등을 포함한 예시를 보여줍니다.

## 5. Conclusion
본 논문의 결과는 조건부 adversarial network가 많은 imag-to-image 변환 작업, 특히 구조화된 그래픽 출력을 포함하는 작업에 유망한 접근 방식임을 시사합니다.

---
추가글
