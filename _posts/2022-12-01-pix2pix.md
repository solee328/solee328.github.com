---
layout: post
title: Pix2Pix(1) - 논문 분석
# subtitle:
categories: gan
tags: [pix2pix, 생성 모델, 논문 분석]
# sidebar: []
use_math: true
---

## 소개
Pair 데이터를 사용한 생성 모델인 Pix2Pix

논문 모든 내용을 번역하지 않고 중요한 부분들을 요약해 번역 -> 의역 존재


## Abstract
image-to-image 변환 문제에 대한 범용 솔루션으로 conditional adversarial networks(이후 조건부 GAN으로 대체 사용하겠습니다)를 사용하며 입력 이미지에서 출력 이미지로의 매핑을 학습하기 위한 loss 함수를 사용한다. label map, edge map에서 개체를 합성 / 재구성하고 이미지를 colorizing에 효과적이라는 것을 보여준다.

## 1. Introduction
이미지 처리, 컴퓨터 그래픽스, 컴퓨터 비전의 많은 과제들은 입력 이미지를 출력 이미지로 변환하는 것에 대해 질문을 던진다. 장면(이미지)는 RGB image, gradient field, edge map, semantic label map 등 다양하게 렌더링될 수 있다. 자동 언어 번역과 유사한 자동 image-to-image 변환을 충분한 학습 데이터가 주어진다면 한 장면의 표현을 다른 장면으로 변환하는 작업으로 정의할 수 있다. 과거의 이런 과제들은 각각 별도의 특수 목적의 시스템으로 처리되었지만 본 논문에서 우리의 목표는 이러한 모든 문제에 대한 공통된 프레임워크를 개발하는 것이다.<br>
convolutional neural nets(CNNs)로 다양한 이미지 예측 문제를 해결하는 방향으로 연구들이 진행되고 있고 CNNs는 loss 함수를 최소화하는 방향으로 학습한다. 하지만 효과적인 loss 함수를 만드는 것은 아직 많은 수작업이 필요하다. 단순한 접근 방식으로 CNN에게 예측(predicted)과 실제(ground truth)의 유클리디안 거리를 최소화하는 것은 흐릿한 결과를 생성하는 경향이 있다. 유클리디안 거리를 최소화하는 것은 결과의 모든 값을 평균화하는 것이기 때문이 흐릿한 결과를 유도하기 때문이다.<br>
'현실과 구별할 수 없는 출력을 만든다'와 같이 고수준의 목표를 명시하는 대신 목표를 달성하기 위한 적절한 loss 함수를 학습할 수 있다면 매우 바람직할 것이다. 다행히 최근 제안된 Generative Adversarial Networks(GANs)에 의해 수행할 수 있다. GANs는 출력이 실제인지 가짜인지 구별하는 판별 모델 loss를 학습하는 동시에 이 loss를 최소화하기 위한 생성 모델 또한 학습한다. 흐릿한 이미지는 명백히 가짜로 보이기 때문에 GANs는 흐릿한 이미지를 가짜로 판별하고 생성 모델은 더 선명한 이미지를 생성하게 될 것이다.<br>
본 논문에서 우리는 GANs의 조건 설정을 탐구한다. GAN이 데이터 생성 모델을 학습하는 것처럼 조건부 GANs(cGAN)은 조건부 생성 모델을 학습한다. cGAN은 조건부 입력 이미지에 따라 출력 이미지를 생성하는 image-to-image 변환에 적합하다.<br>
조건부 GANs가 다양한 과제들에 대해 합리적인 결과를 생성한다는 것을 입증한다. 코드는  https://github.com/phillipi/pix2pix 에서 확인할 수 있다.

## 2. Related work
Structed losses for image modeling, Conditional GANs와 관련된 논문들이 열거되어 있습니다.
Structed losses for image modeling에서 image-to-image 변환 문제는 주로 per-pixel classification이나 regression으로 수식화하고 문제를 해결했으나 조건부 GAN은 loss를 학습한다는 점에서 기존 수식 모델과는 다르고 출력과 정답의 오차가 있는 구조에 대해 loss를 이용해 불이익을 줄 수 있다는 점이 나와있습니다.<br>
Conditional GANs에서는                                                                         


## 3. Method
### 3.1. Objective

### 3.2. Network architectures
#### 3.2.1 Generator with skips

#### 3.2.2 Markovian discriminator (PatchGAN)

### 3.3. Optimization and inference

## 4. Experiments
Data requirements and speed

### 4.1. Evaluation metrics

AMT perceptual studies

"FCN score"

### 4.2. Analysis of the objective function

colorfulness

### 4.3. Analysis of the generator architecture

### 4.4. From PixelGANs to PatchGANs to ImageGANs
Fully-convolutional translation


### 4.5. Perceptual validation


### 4.6. Semantic segmentation

### 4.7. Community-driven Research


## 5. Conclusion
